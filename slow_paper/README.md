# Slow paper

## Read recent papers and review with making presentations.


| 논문 제목                                                     | 논문 URL                                       | 일정           | 
|-------------------------------------------------------------|-----------------------------------------------|---------------|
| Attention is All You Need (Transformer)                    | [link](https://arxiv.org/abs/1706.03762)      | 09/22(금), 09/25(월) |
| LoRA: Low-Rank Adaptation of Large Language Models (LoRA)  | [link](https://arxiv.org/abs/2106.09685)      | 09/26(화), 09/27(수) |
| LLM.int8(): 8-bit Matrix Multiplication for Transformers... | [link](https://arxiv.org/abs/2208.07339)      | 10/4(수), 10/5(목)   |
| QLoRA: Efficient Finetuning of Quantized LLMs (QLoRA)      | [link](https://arxiv.org/abs/2305.14314)      | 10/6(금), 10/10(화)  | 
