{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9660ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel\n"
     ]
    }
   ],
   "source": [
    "%cd /aiffel\n",
    "\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718fd84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gpt_neox to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at aiffel/KoChatGPT/output_2_SFT were not used when initializing GPT2LMHeadModel: ['gpt_neox.layers.11.input_layernorm.weight', 'gpt_neox.layers.1.post_attention_layernorm.bias', 'gpt_neox.layers.4.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.21.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.16.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.10.attention.dense.bias', 'gpt_neox.layers.4.attention.dense.bias', 'gpt_neox.layers.13.attention.query_key_value.weight', 'gpt_neox.layers.14.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.21.attention.dense.weight', 'gpt_neox.layers.8.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.11.attention.masked_bias', 'gpt_neox.layers.20.post_attention_layernorm.weight', 'gpt_neox.layers.22.attention.bias', 'gpt_neox.layers.5.attention.rotary_emb.inv_freq', 'gpt_neox.layers.6.post_attention_layernorm.bias', 'gpt_neox.layers.10.attention.masked_bias', 'gpt_neox.layers.9.mlp.dense_4h_to_h.weight', 'gpt_neox.final_layer_norm.bias', 'gpt_neox.embed_in.weight', 'gpt_neox.layers.2.input_layernorm.bias', 'gpt_neox.layers.1.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.15.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.2.attention.masked_bias', 'gpt_neox.layers.10.attention.query_key_value.weight', 'gpt_neox.layers.13.input_layernorm.bias', 'gpt_neox.layers.8.attention.query_key_value.weight', 'gpt_neox.layers.5.post_attention_layernorm.bias', 'gpt_neox.layers.5.attention.query_key_value.bias', 'gpt_neox.layers.13.attention.rotary_emb.inv_freq', 'gpt_neox.layers.2.attention.dense.bias', 'gpt_neox.layers.18.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.0.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.13.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.20.attention.bias', 'gpt_neox.layers.3.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.4.post_attention_layernorm.bias', 'gpt_neox.layers.5.attention.query_key_value.weight', 'gpt_neox.layers.11.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.19.attention.masked_bias', 'gpt_neox.layers.22.attention.masked_bias', 'gpt_neox.layers.20.attention.rotary_emb.inv_freq', 'gpt_neox.layers.8.attention.dense.weight', 'gpt_neox.layers.13.attention.bias', 'gpt_neox.layers.4.attention.query_key_value.bias', 'gpt_neox.layers.10.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.16.post_attention_layernorm.bias', 'gpt_neox.layers.5.attention.dense.weight', 'gpt_neox.layers.6.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.9.input_layernorm.weight', 'gpt_neox.layers.14.attention.dense.weight', 'gpt_neox.layers.3.input_layernorm.bias', 'gpt_neox.layers.1.attention.masked_bias', 'gpt_neox.layers.0.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.3.input_layernorm.weight', 'gpt_neox.layers.20.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.2.attention.bias', 'gpt_neox.layers.17.attention.bias', 'gpt_neox.layers.9.attention.query_key_value.bias', 'gpt_neox.layers.8.attention.bias', 'gpt_neox.layers.16.post_attention_layernorm.weight', 'embed_out.weight', 'gpt_neox.layers.22.post_attention_layernorm.weight', 'gpt_neox.layers.19.attention.query_key_value.weight', 'gpt_neox.layers.15.attention.bias', 'gpt_neox.layers.18.attention.rotary_emb.inv_freq', 'gpt_neox.layers.3.attention.dense.bias', 'gpt_neox.layers.22.input_layernorm.bias', 'gpt_neox.layers.21.post_attention_layernorm.bias', 'gpt_neox.layers.13.attention.query_key_value.bias', 'gpt_neox.layers.4.attention.bias', 'gpt_neox.layers.5.attention.bias', 'gpt_neox.layers.21.post_attention_layernorm.weight', 'gpt_neox.layers.2.post_attention_layernorm.weight', 'gpt_neox.layers.23.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.12.attention.dense.bias', 'gpt_neox.layers.8.attention.dense.bias', 'gpt_neox.layers.8.post_attention_layernorm.bias', 'gpt_neox.layers.5.input_layernorm.weight', 'gpt_neox.layers.17.post_attention_layernorm.weight', 'gpt_neox.layers.7.attention.dense.bias', 'gpt_neox.layers.4.input_layernorm.weight', 'gpt_neox.layers.9.attention.query_key_value.weight', 'gpt_neox.layers.11.attention.dense.weight', 'gpt_neox.layers.0.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.19.post_attention_layernorm.weight', 'gpt_neox.layers.21.input_layernorm.weight', 'gpt_neox.layers.10.input_layernorm.weight', 'gpt_neox.layers.16.input_layernorm.bias', 'gpt_neox.layers.13.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.14.attention.query_key_value.weight', 'gpt_neox.layers.11.post_attention_layernorm.bias', 'gpt_neox.layers.15.input_layernorm.bias', 'gpt_neox.layers.17.attention.rotary_emb.inv_freq', 'gpt_neox.layers.20.attention.masked_bias', 'gpt_neox.layers.21.attention.dense.bias', 'gpt_neox.layers.22.input_layernorm.weight', 'gpt_neox.layers.22.attention.query_key_value.weight', 'gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.10.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.15.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.12.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.16.attention.dense.weight', 'gpt_neox.layers.1.attention.dense.weight', 'gpt_neox.layers.1.input_layernorm.weight', 'gpt_neox.layers.1.post_attention_layernorm.weight', 'gpt_neox.layers.15.attention.query_key_value.bias', 'gpt_neox.layers.15.attention.rotary_emb.inv_freq', 'gpt_neox.layers.15.post_attention_layernorm.weight', 'gpt_neox.layers.0.input_layernorm.bias', 'gpt_neox.layers.6.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.9.post_attention_layernorm.bias', 'gpt_neox.layers.19.attention.bias', 'gpt_neox.layers.8.attention.masked_bias', 'gpt_neox.layers.17.input_layernorm.weight', 'gpt_neox.layers.18.attention.dense.weight', 'gpt_neox.layers.21.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.9.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.21.attention.query_key_value.weight', 'gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.16.attention.query_key_value.bias', 'gpt_neox.layers.9.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.22.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.8.post_attention_layernorm.weight', 'gpt_neox.layers.3.attention.dense.weight', 'gpt_neox.layers.19.input_layernorm.bias', 'gpt_neox.layers.10.input_layernorm.bias', 'gpt_neox.layers.23.input_layernorm.bias', 'gpt_neox.layers.0.attention.dense.weight', 'gpt_neox.layers.20.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.21.attention.masked_bias', 'gpt_neox.layers.14.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.16.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.23.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.8.input_layernorm.bias', 'gpt_neox.layers.13.attention.dense.weight', 'gpt_neox.layers.19.post_attention_layernorm.bias', 'gpt_neox.layers.7.input_layernorm.weight', 'gpt_neox.layers.9.attention.dense.bias', 'gpt_neox.layers.12.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.8.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.9.post_attention_layernorm.weight', 'gpt_neox.layers.2.attention.query_key_value.bias', 'gpt_neox.layers.12.input_layernorm.bias', 'gpt_neox.layers.19.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.14.attention.rotary_emb.inv_freq', 'gpt_neox.layers.14.input_layernorm.bias', 'gpt_neox.layers.20.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.12.attention.query_key_value.weight', 'gpt_neox.layers.17.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.12.attention.rotary_emb.inv_freq', 'gpt_neox.layers.2.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.18.attention.dense.bias', 'gpt_neox.layers.3.attention.rotary_emb.inv_freq', 'gpt_neox.layers.0.attention.bias', 'gpt_neox.layers.15.attention.masked_bias', 'gpt_neox.layers.18.attention.masked_bias', 'gpt_neox.layers.19.input_layernorm.weight', 'gpt_neox.layers.21.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.4.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.20.attention.dense.weight', 'gpt_neox.layers.23.attention.dense.bias', 'gpt_neox.layers.6.attention.query_key_value.bias', 'gpt_neox.layers.6.post_attention_layernorm.weight', 'gpt_neox.layers.19.attention.dense.bias', 'gpt_neox.layers.22.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.7.attention.query_key_value.bias', 'gpt_neox.layers.1.attention.dense.bias', 'gpt_neox.layers.2.attention.rotary_emb.inv_freq', 'gpt_neox.layers.19.attention.rotary_emb.inv_freq', 'gpt_neox.layers.17.attention.masked_bias', 'gpt_neox.layers.22.attention.dense.bias', 'gpt_neox.layers.10.attention.bias', 'gpt_neox.layers.15.attention.dense.bias', 'gpt_neox.layers.15.post_attention_layernorm.bias', 'gpt_neox.layers.5.attention.dense.bias', 'gpt_neox.layers.14.attention.bias', 'gpt_neox.layers.10.attention.rotary_emb.inv_freq', 'gpt_neox.layers.20.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.19.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.6.attention.dense.bias', 'gpt_neox.layers.4.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.14.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.15.input_layernorm.weight', 'gpt_neox.layers.19.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.13.post_attention_layernorm.bias', 'gpt_neox.layers.7.attention.query_key_value.weight', 'gpt_neox.layers.12.attention.dense.weight', 'gpt_neox.layers.1.attention.bias', 'gpt_neox.layers.9.attention.rotary_emb.inv_freq', 'gpt_neox.layers.14.attention.query_key_value.bias', 'gpt_neox.layers.18.post_attention_layernorm.weight', 'gpt_neox.layers.18.attention.query_key_value.bias', 'gpt_neox.layers.18.input_layernorm.bias', 'gpt_neox.layers.1.attention.rotary_emb.inv_freq', 'gpt_neox.layers.5.post_attention_layernorm.weight', 'gpt_neox.layers.1.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.8.attention.query_key_value.bias', 'gpt_neox.layers.1.input_layernorm.bias', 'gpt_neox.layers.4.post_attention_layernorm.weight', 'gpt_neox.layers.12.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.14.input_layernorm.weight', 'gpt_neox.layers.14.attention.masked_bias', 'gpt_neox.layers.13.input_layernorm.weight', 'gpt_neox.layers.12.attention.bias', 'gpt_neox.layers.2.attention.query_key_value.weight', 'gpt_neox.layers.9.attention.masked_bias', 'gpt_neox.layers.12.post_attention_layernorm.bias', 'gpt_neox.layers.7.attention.dense.weight', 'gpt_neox.layers.2.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.7.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.5.input_layernorm.bias', 'gpt_neox.layers.15.attention.dense.weight', 'gpt_neox.layers.23.attention.rotary_emb.inv_freq', 'gpt_neox.layers.20.attention.query_key_value.weight', 'gpt_neox.layers.21.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.10.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.15.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.22.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.23.attention.query_key_value.weight', 'gpt_neox.layers.23.post_attention_layernorm.weight', 'gpt_neox.layers.0.post_attention_layernorm.weight', 'gpt_neox.layers.17.attention.query_key_value.weight', 'gpt_neox.layers.22.attention.rotary_emb.inv_freq', 'gpt_neox.layers.7.post_attention_layernorm.weight', 'gpt_neox.layers.19.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.17.attention.query_key_value.bias', 'gpt_neox.layers.7.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.11.attention.bias', 'gpt_neox.layers.1.attention.query_key_value.weight', 'gpt_neox.layers.16.attention.masked_bias', 'gpt_neox.layers.8.attention.rotary_emb.inv_freq', 'gpt_neox.layers.16.attention.bias', 'gpt_neox.layers.2.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.6.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.14.post_attention_layernorm.weight', 'gpt_neox.layers.7.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.13.post_attention_layernorm.weight', 'gpt_neox.layers.18.input_layernorm.weight', 'gpt_neox.layers.22.attention.query_key_value.bias', 'gpt_neox.layers.10.attention.dense.weight', 'gpt_neox.layers.20.post_attention_layernorm.bias', 'gpt_neox.layers.17.post_attention_layernorm.bias', 'gpt_neox.layers.18.post_attention_layernorm.bias', 'gpt_neox.layers.16.input_layernorm.weight', 'gpt_neox.layers.11.input_layernorm.bias', 'gpt_neox.layers.18.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.11.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.3.attention.bias', 'gpt_neox.layers.16.attention.query_key_value.weight', 'gpt_neox.layers.7.attention.masked_bias', 'gpt_neox.layers.0.attention.dense.bias', 'gpt_neox.layers.9.attention.dense.weight', 'gpt_neox.layers.3.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.4.attention.masked_bias', 'gpt_neox.layers.6.input_layernorm.bias', 'gpt_neox.layers.20.attention.dense.bias', 'gpt_neox.layers.4.input_layernorm.bias', 'gpt_neox.layers.17.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.0.attention.rotary_emb.inv_freq', 'gpt_neox.layers.6.attention.bias', 'gpt_neox.layers.19.attention.query_key_value.bias', 'gpt_neox.layers.3.post_attention_layernorm.weight', 'gpt_neox.layers.17.attention.dense.weight', 'gpt_neox.layers.23.attention.masked_bias', 'gpt_neox.layers.16.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.5.attention.masked_bias', 'gpt_neox.layers.10.attention.query_key_value.bias', 'gpt_neox.layers.6.attention.rotary_emb.inv_freq', 'gpt_neox.layers.6.attention.query_key_value.weight', 'gpt_neox.layers.11.attention.rotary_emb.inv_freq', 'gpt_neox.layers.18.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.0.post_attention_layernorm.bias', 'gpt_neox.layers.20.input_layernorm.bias', 'gpt_neox.layers.8.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.11.attention.query_key_value.bias', 'gpt_neox.layers.15.attention.query_key_value.weight', 'gpt_neox.layers.9.attention.bias', 'gpt_neox.layers.1.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.3.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.23.post_attention_layernorm.bias', 'gpt_neox.layers.0.attention.masked_bias', 'gpt_neox.layers.23.attention.bias', 'gpt_neox.layers.3.attention.masked_bias', 'gpt_neox.layers.1.attention.query_key_value.bias', 'gpt_neox.layers.10.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.9.input_layernorm.bias', 'gpt_neox.layers.11.attention.dense.bias', 'gpt_neox.layers.1.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.23.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.4.attention.rotary_emb.inv_freq', 'gpt_neox.layers.18.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.11.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.6.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.2.attention.dense.weight', 'gpt_neox.layers.18.attention.bias', 'gpt_neox.layers.6.attention.masked_bias', 'gpt_neox.layers.10.post_attention_layernorm.weight', 'gpt_neox.layers.23.attention.dense.weight', 'gpt_neox.layers.20.input_layernorm.weight', 'gpt_neox.layers.20.attention.query_key_value.bias', 'gpt_neox.layers.4.attention.query_key_value.weight', 'gpt_neox.layers.23.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.8.input_layernorm.weight', 'gpt_neox.layers.17.attention.dense.bias', 'gpt_neox.layers.7.attention.rotary_emb.inv_freq', 'gpt_neox.layers.3.post_attention_layernorm.bias', 'gpt_neox.layers.17.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.2.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.23.attention.query_key_value.bias', 'gpt_neox.layers.13.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.12.attention.masked_bias', 'gpt_neox.layers.0.input_layernorm.weight', 'gpt_neox.layers.3.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.6.attention.dense.weight', 'gpt_neox.layers.13.attention.dense.bias', 'gpt_neox.layers.22.post_attention_layernorm.bias', 'gpt_neox.layers.2.post_attention_layernorm.bias', 'gpt_neox.layers.11.attention.query_key_value.weight', 'gpt_neox.layers.3.attention.query_key_value.weight', 'gpt_neox.layers.22.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.16.attention.dense.bias', 'gpt_neox.layers.7.post_attention_layernorm.bias', 'gpt_neox.layers.13.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.2.input_layernorm.weight', 'gpt_neox.layers.21.attention.rotary_emb.inv_freq', 'gpt_neox.layers.14.attention.dense.bias', 'gpt_neox.layers.14.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.4.attention.dense.weight', 'gpt_neox.layers.6.input_layernorm.weight', 'gpt_neox.layers.7.input_layernorm.bias', 'gpt_neox.layers.21.input_layernorm.bias', 'gpt_neox.layers.21.attention.bias', 'gpt_neox.layers.8.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.12.post_attention_layernorm.weight', 'gpt_neox.layers.15.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.17.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.19.attention.dense.weight', 'gpt_neox.layers.13.attention.masked_bias', 'gpt_neox.layers.18.attention.query_key_value.weight', 'gpt_neox.layers.11.post_attention_layernorm.weight', 'gpt_neox.layers.7.attention.bias', 'gpt_neox.layers.10.post_attention_layernorm.bias', 'gpt_neox.layers.7.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.16.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.0.attention.query_key_value.weight', 'gpt_neox.layers.12.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.16.attention.rotary_emb.inv_freq', 'gpt_neox.layers.12.attention.query_key_value.bias', 'gpt_neox.layers.12.input_layernorm.weight', 'gpt_neox.layers.22.attention.dense.weight', 'gpt_neox.layers.0.attention.query_key_value.bias', 'gpt_neox.layers.0.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.3.attention.query_key_value.bias', 'gpt_neox.final_layer_norm.weight', 'gpt_neox.layers.11.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.14.post_attention_layernorm.bias', 'gpt_neox.layers.17.input_layernorm.bias', 'gpt_neox.layers.4.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.9.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.21.attention.query_key_value.bias', 'gpt_neox.layers.23.input_layernorm.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='aiffel/KoChatGPT/output_2_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='aiffel/KoChatGPT/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'EleutherAI/polyglot-ko-1.3b', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "405c72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19755c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e7c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d721b9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   44,    87,  3628, 12504,  5862,  9244,  6723,  5145,    75,  4541,\n",
      "          7789, 10101, 22253,  2818, 10903,    87, 15075,  7135, 18190, 13384,\n",
      "           224, 16683, 10903,    87, 15075,  7135,  6522, 16736,    17]],\n",
      "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7f02c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d059308",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=2, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=64,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fa6acab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train epoch [1/1]:   0%|          | 0/4 [00:01<?, ?it/s]\u001b[A\n",
      "Episode [1/10]:   0%|          | 0/1 [00:22<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 trainer.fit(list_prompt,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>num_episodes=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>max_timesteps=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>update_timesteps=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/aiffel/chatgpt/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">118</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fit</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._on_make_experience_end(experience)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.replay_buffer.append(experience)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> time % update_timesteps == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>118 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._learn()                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.replay_buffer.clear()                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._on_episode_end(episode)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._on_fit_end()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/aiffel/chatgpt/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">93</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_learn</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 90 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> experience <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> pbar:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._on_learn_batch_start()                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 92 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>experience.to_device(device)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 93 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>metrics = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(experience)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._on_learn_batch_end(metrics, experience)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>pbar.set_postfix(metrics)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 96 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._on_learn_epoch_end(epoch)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/aiffel/chatgpt/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ppo.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">88</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 85 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   </span>experience.action_log_probs,                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 86 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   </span>experience.advantages,                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 87 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   │   │   </span>action_mask=experience.action_mask)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 88 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.strategy.backward(actor_loss, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.actor, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.actor_optim)                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 89 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.strategy.optimizer_step(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.actor_optim)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 90 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.actor_optim.zero_grad()                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/aiffel/chatgpt/trainer/strategies/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">naive.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">19</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, loss: torch.Tensor, model: nn.Module, optimizer: optim.Optimizer,    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>19 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss.backward()                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">optimizer_step</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer: optim.Optimizer, **kwargs) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>optimizer.step()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">396</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 393 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>retain_graph=retain_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 394 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 395 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>inputs=inputs)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 396 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=input  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 397 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 398 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">register_hook</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hook):                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 399 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">r\"\"\"Registers a backward hook.</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">173</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">171 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>173 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.76</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.89</span> GiB already \n",
       "allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.75</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.07</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 trainer.fit(list_prompt,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[2m│   │   │   \u001b[0mnum_episodes=\u001b[94m10\u001b[0m,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[2m│   │   │   \u001b[0mmax_timesteps=\u001b[94m1\u001b[0m,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[2m│   │   │   \u001b[0mupdate_timesteps=\u001b[94m1\u001b[0m)                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/aiffel/chatgpt/trainer/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m118\u001b[0m in \u001b[92mfit\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._on_make_experience_end(experience)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.replay_buffer.append(experience)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m time % update_timesteps == \u001b[94m0\u001b[0m:                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m118 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._learn()                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.replay_buffer.clear()                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._on_episode_end(episode)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._on_fit_end()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/aiffel/chatgpt/trainer/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m93\u001b[0m in \u001b[92m_learn\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m experience \u001b[95min\u001b[0m pbar:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._on_learn_batch_start()                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 92 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mexperience.to_device(device)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 93 \u001b[2m│   │   │   │   │   \u001b[0mmetrics = \u001b[96mself\u001b[0m.training_step(experience)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._on_learn_batch_end(metrics, experience)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mpbar.set_postfix(metrics)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 96 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._on_learn_epoch_end(epoch)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/aiffel/chatgpt/trainer/\u001b[0m\u001b[1;33mppo.py\u001b[0m:\u001b[94m88\u001b[0m in \u001b[92mtraining_step\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 85 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   \u001b[0mexperience.action_log_probs,                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 86 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   \u001b[0mexperience.advantages,                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 87 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   \u001b[0maction_mask=experience.action_mask)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 88 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.strategy.backward(actor_loss, \u001b[96mself\u001b[0m.actor, \u001b[96mself\u001b[0m.actor_optim)                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 89 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.strategy.optimizer_step(\u001b[96mself\u001b[0m.actor_optim)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.actor_optim.zero_grad()                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/aiffel/chatgpt/trainer/strategies/\u001b[0m\u001b[1;33mnaive.py\u001b[0m:\u001b[94m19\u001b[0m in \u001b[92mbackward\u001b[0m                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mbackward\u001b[0m(\u001b[96mself\u001b[0m, loss: torch.Tensor, model: nn.Module, optimizer: optim.Optimizer,    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m19 \u001b[2m│   │   \u001b[0mloss.backward()                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92moptimizer_step\u001b[0m(\u001b[96mself\u001b[0m, optimizer: optim.Optimizer, **kwargs) -> \u001b[94mNone\u001b[0m:                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.step()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m396\u001b[0m in \u001b[92mbackward\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 393 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mretain_graph=retain_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 394 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 395 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs)                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 396 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=input  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 397 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 398 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mregister_hook\u001b[0m(\u001b[96mself\u001b[0m, hook):                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 399 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\"\"Registers a backward hook.\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m173\u001b[0m in \u001b[92mbackward\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m173 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m176 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m48.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m14.76\u001b[0m GiB total capacity; \u001b[1;36m13.89\u001b[0m GiB already \n",
       "allocated; \u001b[1;36m19.75\u001b[0m MiB free; \u001b[1;36m14.07\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf3155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.model.save_pretrained('aiffel/KoChatGPT/output_3_PPO_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aed516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40862f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
